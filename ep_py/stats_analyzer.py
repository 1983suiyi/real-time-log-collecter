#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨ç»Ÿè®¡åˆ†æè„šæœ¬ï¼šæ”¯æŒå¤šç§å­—æ®µç»„åˆçš„ç»Ÿè®¡åˆ†æ
ç”¨æ³•ï¼špython stats_analyzer.py <csv_file_path> [--stats-configs <json_config>]
"""

import pandas as pd
import sys
import os
import json
import argparse
from collections import Counter
from tabulate import tabulate

def analyze_stats(csv_file_path, stats_configs=None, top_n=20):
    """
    åˆ†æCSVæ–‡ä»¶ä¸­æŒ‡å®šå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        csv_file_path (str): CSVæ–‡ä»¶è·¯å¾„
        stats_configs (list): ç»Ÿè®¡é…ç½®åˆ—è¡¨
        top_n (int): æ˜¾ç¤ºå‰Nä¸ªç»“æœ
    
    Returns:
        dict: ç»Ÿè®¡ç»“æœ
    """
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file_path)
        
        # é»˜è®¤é…ç½®ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
        if stats_configs is None:
            stats_configs = [{
                'name': 'service_error_stats',
                'fields': ['httpServiceName', 'errorCode'],
                'description': 'httpServiceNameä¸errorCodeè”åˆç»Ÿè®¡'
            }]
        
        print(f"ğŸ“ åˆ†ææ–‡ä»¶: {csv_file_path}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(df)}")
        print("\n" + "="*80)
        print("ğŸ“ˆ æ•°æ®ç»Ÿè®¡åˆ†æç»“æœ")
        print("="*80)
        
        all_results = {}
        
        # ä¸ºæ¯ä¸ªç»Ÿè®¡é…ç½®è¿›è¡Œåˆ†æ
        for config in stats_configs:
            config_name = config['name']
            fields = config['fields']
            description = config.get('description', config_name)
            
            print(f"\n{'='*60}")
            print(f"ğŸ“Š {description} ({config_name})")
            print(f"{'='*60}")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = [col for col in fields if col not in df.columns]
            
            if missing_columns:
                print(f"âš ï¸  è­¦å‘Šï¼šCSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                print(f"ğŸ“‹ å¯ç”¨çš„åˆ—: {list(df.columns)}")
                continue
            
            # è¿‡æ»¤æ‰ç©ºå€¼
            df_filtered = df.dropna(subset=fields)
            
            print(f"ğŸ“ˆ æœ‰æ•ˆè®°å½•æ•°ï¼ˆæ‰€æœ‰å­—æ®µéç©ºï¼‰: {len(df_filtered)}")
            
            if len(df_filtered) == 0:
                print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆè®°å½•å¯ä¾›åˆ†æ")
                continue
            
            # é…ç½®è¯¦ç»†ä¿¡æ¯
            config_data = [
                ["ç»Ÿè®¡å­—æ®µ", ", ".join(fields)],
                ["æœ‰æ•ˆè®°å½•æ•°", len(df_filtered)],
            ]
            
            # è®¡ç®—æ¯ä¸ªå­—æ®µçš„å”¯ä¸€å€¼æ•°é‡
            for field in fields:
                unique_count = df_filtered[field].nunique()
                config_data.append([f"å”¯ä¸€{field}æ•°é‡", unique_count])
            
            print("\nğŸ“‹ é…ç½®ä¿¡æ¯:")
            print(tabulate(config_data, headers=['æŒ‡æ ‡', 'æ•°å€¼'], tablefmt='grid'))
            
            # è”åˆç»Ÿè®¡ï¼ˆå¤šå­—æ®µç»„åˆï¼‰
            if len(fields) > 1:
                combined_stats = df_filtered.groupby(fields).size().reset_index(name='count')
                combined_stats = combined_stats.sort_values('count', ascending=False)
                
                config_data.append(["å”¯ä¸€ç»„åˆæ•°é‡", len(combined_stats)])
                
                print(f"\nğŸ“Š {description}è”åˆç»Ÿè®¡ (å‰{min(top_n, len(combined_stats))}ä¸ª):")
                table_data = []
                for _, row in combined_stats.head(top_n).iterrows():
                    row_data = [row[field] for field in fields] + [row['count']]
                    table_data.append(row_data)
                
                headers = fields + ['Count']
                print(tabulate(table_data, headers=headers, tablefmt='grid', stralign='left'))
                
                # ä¿å­˜è”åˆç»Ÿè®¡åˆ°æ–‡ä»¶
                output_dir = os.path.dirname(csv_file_path)
                stats_file = os.path.join(output_dir, f'{config_name}_stats.csv')
                combined_stats.to_csv(stats_file, index=False, encoding='utf-8')
                print(f"ğŸ’¾ è”åˆç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
                
                all_results[config_name] = {
                    'config': config,
                    'valid_records': len(df_filtered),
                    'combined_stats': combined_stats
                }
            
            # å•ä¸ªå­—æ®µç»Ÿè®¡
            for field in fields:
                field_stats = df_filtered[field].value_counts()
                print(f"\nğŸ“ˆ {field}ç»Ÿè®¡ (å‰{min(top_n//2, len(field_stats))}ä¸ª):")
                field_table = [[value, count] for value, count in field_stats.head(top_n//2).items()]
                print(tabulate(field_table, headers=[field, 'Count'], 
                              tablefmt='grid', stralign='left'))
                
                if config_name not in all_results:
                    all_results[config_name] = {
                        'config': config,
                        'valid_records': len(df_filtered)
                    }
                
                all_results[config_name][f'{field}_stats'] = field_stats
        
        return all_results
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file_path}")
        return None
    except pd.errors.EmptyDataError:
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {csv_file_path} ä¸ºç©º")
        return None
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šå¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ - {str(e)}")
        return None

def main():
    parser = argparse.ArgumentParser(description='é€šç”¨ç»Ÿè®¡åˆ†æè„šæœ¬')
    parser.add_argument('csv_file', help='è¦åˆ†æçš„CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--stats-configs', type=str, help='ç»Ÿè®¡é…ç½®JSONå­—ç¬¦ä¸²')
    parser.add_argument('--top', type=int, default=20, help='æ˜¾ç¤ºå‰Nä¸ªç»“æœï¼ˆé»˜è®¤20ï¼‰')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.csv_file):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {args.csv_file} ä¸å­˜åœ¨")
        sys.exit(1)
    
    # è§£æç»Ÿè®¡é…ç½®
    stats_configs = None
    if args.stats_configs:
        try:
            stats_configs = json.loads(args.stats_configs)
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯ï¼šæ— æ³•è§£æç»Ÿè®¡é…ç½®JSON - {str(e)}")
            sys.exit(1)
    
    print(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {args.csv_file}")
    print("="*80)
    
    result = analyze_stats(args.csv_file, stats_configs, args.top)
    
    if result is None:
        sys.exit(1)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == '__main__':
    main()